{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from SAC.Agent import Agent\n",
    "from environment import Env\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow, show   \n",
    "from matplotlib import rcParams\n",
    "from IPython import display\n",
    "import time\n",
    "import copy\n",
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Training Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class Agent_Training():\n",
    "    def __init__(self, image_dims, seed = 1):\n",
    "        # Initialisations\n",
    "        self.seed = seed\n",
    "        self.image_dims = image_dims\n",
    "        self.n_episodes = 50\n",
    "        self.episode_len = 200\n",
    "        self.seq_len = 4\n",
    "        self.env = Env(self.image_dims, self.seed)\n",
    "        self.n_actions = self.env.action_space.shape[0]\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def update_sequences(self, seq_observation, seq_action, observation, actions):\n",
    "        seq_observation = np.roll(seq_observation, -1, axis=0)\n",
    "        seq_observation[-1] = observation\n",
    "        seq_action = np.roll(seq_action, -1, axis=0)\n",
    "        seq_action[-1] = actions\n",
    "\n",
    "        return seq_observation, seq_action\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def initial_window(self):\n",
    "        observation = self.env.reset()\n",
    "        seq_observation = []\n",
    "        seq_observation_ = []\n",
    "        seq_actions = []     \n",
    "           \n",
    "        for i in range(self.seq_len): \n",
    "            action = [0 for i in range(self.n_actions)]\n",
    "            observation_, obs = self.env.step(action)\n",
    "            seq_observation.append(observation)\n",
    "            seq_observation_.append(observation_)\n",
    "            seq_actions.append(action)\n",
    "            observation = observation_\n",
    "        \n",
    "        return np.array(seq_observation), np.array(seq_observation_), np.array(seq_actions, dtype=np.float64)\n",
    "    \n",
    "\n",
    "    def initial_window_all(self):\n",
    "        final_seq_observation, final_seq_observation_, final_seq_actions = self.initial_window()\n",
    "        return final_seq_observation, final_seq_actions\n",
    "\n",
    "\n",
    "    def test_actor_video(self):\n",
    "        frames = [] # store the frames for the video\n",
    "        observations = []\n",
    "        \n",
    "        with tqdm(total=self.n_episodes*self.episode_len) as pbar:\n",
    "            \n",
    "            for i in range(self.n_episodes):\n",
    "                seq_observation, seq_action = self.initial_window_all()\n",
    "                seq_observation_ = copy.deepcopy(seq_observation)\n",
    "                \n",
    "                for t in range(self.episode_len):  \n",
    "                    action = [0., 0., 0.] # static arms\n",
    "                    \n",
    "                    # get camera views\n",
    "                    view1, view2 = self.env.my_render(dims = self.image_dims) # two different views - front and left. Two images instead of 1, test 1 vs 2 views\n",
    "                    concatenated_image = np.concatenate([view1[:,:,::-1], view2[:,:,::-1]], axis=2).transpose(2, 1, 0) / 255.0 \n",
    "                    frames.append(concatenated_image)\n",
    "                    \n",
    "                    # perform a step\n",
    "                    observation_, obs = self.env.step(action)\n",
    "                    observations.append(obs)\n",
    "                    seq_observation_, seq_action = self.update_sequences(seq_observation_, seq_action, observation_, action)\n",
    "                    seq_observation = seq_observation_\n",
    "                    pbar.update(1)\n",
    "        \n",
    "        return frames, observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get & Process Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dimensions of image\n",
    "dims = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [09:55<00:00, 16.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get the training data\n",
    "self = Agent_Training(image_dims = dims)\n",
    "frames, observations = self.test_actor_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save frames and observations\n",
    "np.save('frames.npy', frames)\n",
    "np.save('observations.npy', observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load frames and observations\n",
    "frames = np.load('frames.npy')\n",
    "observations = np.load('observations.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Front View Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 32, 32, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_717/332606914.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  front_images = torch.tensor(front_images, dtype=torch.float32).to('cpu')\n"
     ]
    }
   ],
   "source": [
    "# get front view images\n",
    "# environment resets every 100 steps\n",
    "front_images = []\n",
    "for i in range(len(frames)):\n",
    "    x = frames[i]\n",
    "    # swap the first and last dimensions\n",
    "    x = np.swapaxes(x, 0, 2)\n",
    "    front_images.append(x[:, :, :3])\n",
    "\n",
    "# convert front_images to a torch tensor\n",
    "front_images = torch.tensor(front_images, dtype=torch.float32).to('cpu')\n",
    "print(front_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualisations of the front view images\n",
    "# down & left = directions of increase\n",
    "i = 0\n",
    "for image in front_images[:200]:\n",
    "    i += 1\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off the axis\n",
    "    plt.pause(0.001)  # Pause for 1 second between images\n",
    "    # clear the current image\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Corresponding Labels (Positions + Velocities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 6])\n"
     ]
    }
   ],
   "source": [
    "# convert observations to a torch tensor\n",
    "observations = torch.tensor(observations, dtype=torch.float32).to('cpu')\n",
    "print(observations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-format data into batches of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 15, 32, 32])\n",
      "torch.Size([2000, 6])\n",
      "torch.Size([2000, 6])\n"
     ]
    }
   ],
   "source": [
    "# reformat image and label data\n",
    "training_images = torch.zeros(int(observations.shape[0]/5), dims, dims, 3*5)\n",
    "position_labels = torch.zeros(int(observations.shape[0]/5), observations.shape[1])\n",
    "velocity_labels = torch.zeros(int(observations.shape[0]/5), observations.shape[1])\n",
    "\n",
    "# reformat data into stacks of 5 images\n",
    "for i in range(int(observations.shape[0]/5)):\n",
    "    # stack the 3 RGB channels of every 5 images together\n",
    "    training_images[i] = torch.cat([front_images[i*5], front_images[i*5+1], front_images[i*5+2], front_images[i*5+3], front_images[i*5+4]], dim=2)\n",
    "    \n",
    "    # get the x & y positions for every 5th image\n",
    "    position_labels[i] = observations[i*5+4]\n",
    "    \n",
    "    # get the x & y velocities between every 4th and 5th images\n",
    "    velocity_labels[i] = observations[i*5+4] - observations[i*5]\n",
    "    \n",
    "# reshape traning_images to (n_samples, 3*5, 32, 32)\n",
    "training_images = training_images.permute(0, 3, 1, 2)\n",
    "\n",
    "print(training_images.shape)\n",
    "print(position_labels.shape)\n",
    "print(velocity_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# save the training data\n",
    "torch.save(training_images, 'training_images32.pt')\n",
    "torch.save(position_labels, 'position_labels32.pt')\n",
    "torch.save(velocity_labels, 'velocity_labels32.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fetchLinux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
