{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a80c1ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from SAC.Agent import Agent\n",
    "from Robot.Environment import Env\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow, show   \n",
    "from matplotlib import rcParams\n",
    "from IPython import display\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02a95870-25e7-4005-86d6-0bf3454cfe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_Training():\n",
    "    def __init__(self, subpolicy, image_dims, seed = 1):\n",
    "        self.seed = seed\n",
    "        self.image_dims = image_dims\n",
    "        self.subpolicy = subpolicy\n",
    "        self.static = False\n",
    "        self.n_epochs = 250\n",
    "        self.n_eval_episodes = 5\n",
    "        self.n_test_episodes = 15\n",
    "        self.episode_len = 100\n",
    "        self.exploration_steps = 1000\n",
    "        self.seq_len = 4\n",
    "        \n",
    "        self.env = Env(self.image_dims, self.seed)\n",
    "        self.n_actions = self.env.action_space.shape[0]\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.agent = Agent(self.image_dims, self.n_actions, self.seq_len, self.env, self.seed) \n",
    "            \n",
    "        if self.subpolicy > 1: self.trained_actors = self.get_trained_actors(target_subpolicy = self.subpolicy)\n",
    "\n",
    "            \n",
    "    \"\"\"\n",
    "    REMEMBER - WE PROCESS THE IMAGES INSIDE THE ENV CLASS, WHERE WE CONCATENATE THE TWO IMAGES (FRONT AND LEFT CAMERA) \n",
    "    AS 6 CHANNELS, RESHAPE TO (CHANNEL, HEIGHT, WEIDTH), AND THEN NORMALIZE (IMAGES / 255) - see: def my_render(image_dims)\n",
    "    # already done in the environment class\n",
    "    \"\"\"\n",
    "            \n",
    "        #### function to \n",
    "    def integrate_weights(self):\n",
    "        \"\"\"\n",
    "        - Specify which layers to transfer, and which to freeze\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_tensor(self, observation):\n",
    "        observation = torch.Tensor(observation).unsqueeze(0).to(self.device)\n",
    "        return observation\n",
    "    \n",
    "    def get_trained_actors(self, target_subpolicy):\n",
    "        \"\"\"\n",
    "        - Get the trained actor either to test, or to get to the initial state of the next subpolicy\n",
    "        \"\"\"\n",
    "        trained_actors = []\n",
    "        for i in range(1, target_subpolicy):\n",
    "            new_agent = Agent(self.image_dims, self.n_actions, self.seq_len, self.env, self.seed) \n",
    "            trained_actor = new_agent.actor\n",
    "            trained_actor.load_state_dict(torch.load(f'Models/models_sub_{i}/Actor'))\n",
    "            trained_actor.eval()\n",
    "            trained_actors.append(trained_actor)\n",
    "        return trained_actors\n",
    "    \n",
    "    def update_sequences(self, seq_observation, seq_action, observation, actions):\n",
    "        seq_observation = np.roll(seq_observation, -1, axis=0)\n",
    "        seq_observation[-1] = observation\n",
    "        seq_action = np.roll(seq_action, -1, axis=0)\n",
    "        seq_action[-1] = actions\n",
    "        return seq_observation, seq_action\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample_actions(self, seq_observation, seq_action, actor, reparameterize):\n",
    "        action, _ = actor.sample_normal(self.get_tensor(seq_observation), self.get_tensor(seq_action[1:]), reparameterize=reparameterize)\n",
    "        action = action.cpu().detach().numpy()[0]\n",
    "        return action\n",
    "    \n",
    "    def initial_window(self):\n",
    "        \"\"\"\n",
    "        - Get the initial observation sequence for first subpolicy\n",
    "        - We use neutral actions (0) to minimize the interactions with env\n",
    "        \"\"\"\n",
    "        observation = self.env.reset()\n",
    "        seq_observation = []\n",
    "        seq_observation_ = []\n",
    "        seq_actions = []        \n",
    "        for i in range(self.seq_len): \n",
    "            subpolicy = 1\n",
    "            action = [0 for i in range(self.n_actions)]\n",
    "            observation_, _, done, _ = self.env.step(action, subpolicy, self.static)\n",
    "            seq_observation.append(observation)\n",
    "            seq_observation_.append(observation_)\n",
    "            seq_actions.append(action)\n",
    "            observation = observation_\n",
    "        return np.array(seq_observation), np.array(seq_observation_), np.array(seq_actions, dtype=np.float64)\n",
    "    \n",
    "\n",
    "    def initial_window_all(self, subpolicy=None, train=False):\n",
    "        \"\"\"\n",
    "        - Get the initial observation sequence for all subpolicy\n",
    "        \"\"\"\n",
    "        if subpolicy is None: subpolicy = self.subpolicy\n",
    "\n",
    "        if subpolicy == 1:\n",
    "            final_seq_observation, final_seq_observation_, final_seq_actions = self.initial_window()\n",
    "        else:\n",
    "            succesfull = False\n",
    "            for trained_subpolicy in range(1, subpolicy):\n",
    "                trained_actor = self.trained_actors[trained_subpolicy - 1]            \n",
    "                while not succesfull:\n",
    "                    if trained_subpolicy == 1: seq_observation, _, seq_actions = self.initial_window()\n",
    "                    seq_observation_ = copy.deepcopy(seq_observation)\n",
    "                    last_seq_observation = []\n",
    "                    last_seq_observation_ = []\n",
    "                    last_seq_actions = []\n",
    "                    for t in range(self.episode_len):\n",
    "                        action = self.sample_actions(seq_observation, seq_actions, trained_actor, reparameterize=False)\n",
    "                        observation_, _, done, succesfull = self.env.step(action, trained_subpolicy, self.static)\n",
    "                        seq_observation_, seq_actions = self.update_sequences(seq_observation_, seq_actions, observation_, action)\n",
    "                        last_seq_observation.append(seq_observation)\n",
    "                        last_seq_observation_.append(seq_observation_)\n",
    "                        last_seq_actions.append(seq_actions)\n",
    "                        seq_observation = seq_observation_\n",
    "                        if done: break\n",
    "            final_seq_observation ,final_seq_observation_ , final_seq_actions = last_seq_observation[-1], last_seq_observation_[-1], last_seq_actions[-1]\n",
    "        if train:\n",
    "            for s in range(0, self.seq_len - 1):\n",
    "                self.agent.remember(final_seq_observation[s], final_seq_actions[s], None, final_seq_observation_[s], 0)\n",
    "            \n",
    "        return final_seq_observation, final_seq_actions\n",
    "\n",
    "    \n",
    "    def initial_exploration(self):\n",
    "        \"\"\"\n",
    "        - Explore the environomnt to fill in the buffer and exceed the batch size\n",
    "        \"\"\"\n",
    "        seq_observation, seq_action = self.initial_window_all()        \n",
    "        seq_observation_ = copy.deepcopy(seq_observation)\n",
    "        for t in tqdm(range(self.exploration_steps)):\n",
    "            action = self.sample_actions(seq_observation, seq_action, self.agent.actor, reparameterize=True)\n",
    "            observation_, reward, done, _ = self.env.step(action, self.subpolicy, self.static)\n",
    "            seq_observation_, seq_action = self.update_sequences(seq_observation_, seq_action, observation_, action)\n",
    "            self.agent.remember(seq_observation[-1], seq_action[-1], reward, seq_observation_[-1], done)\n",
    "            seq_observation = seq_observation_\n",
    "            if done: \n",
    "                seq_observation, seq_action = self.initial_window_all()   \n",
    "                seq_observation_ = copy.deepcopy(seq_observation)\n",
    "        print('------------ Hey Fella, The Initial Exploration Has Just Finished -----------------')\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        - Train the agent, save metrics for visualization, and save model\n",
    "        \"\"\"\n",
    "        all_mean_rewards, all_actor_loss = [], []\n",
    "        self.initial_exploration() \n",
    "        with tqdm(total = self.n_epochs*episode_len) as pbar:\n",
    "            for epoch in range(self.n_epochs):\n",
    "                seq_observation, seq_action = self.initial_window_all()  \n",
    "                seq_observation_ = copy.deepcopy(seq_observation)\n",
    "                for t in range(self.episode_len):\n",
    "                    action = self.sample_actions(seq_observation, seq_action, self.agent.actor,  reparameterize=True)\n",
    "                    observation_, reward, done, info = self.env.step(action, self.subpolicy, self.static) \n",
    "                    seq_observation_, seq_action = self.update_sequences(seq_observation_, seq_action, observation_, action)\n",
    "                    # ----------- Store Transitions --------------\n",
    "                    self.agent.remember(seq_observation[-1], seq_action[-1], reward, seq_observation_[-1], done) #######  CHANGE HERE FOR BUFFER!!!\n",
    "                    actor_loss = self.agent.learn()\n",
    "                    seq_observation = seq_observation_\n",
    "                    if done: break\n",
    "                    pbar.update(1)\n",
    "\n",
    "                mean_rewards = self.validate_train()\n",
    "                self.agent.save_models(self.subpolicy)\n",
    "                all_mean_rewards.append(mean_rewards)\n",
    "                all_actor_loss.append(actor_loss)\n",
    "                print(f'Epoch: {epoch}, Rewards: {mean_rewards}, Actor Loss: {actor_loss}')\n",
    "\n",
    "                plt.plot(all_mean_rewards)\n",
    "                plt.show()\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "                time.sleep(0.2)\n",
    "        plt.plot(all_mean_rewards)\n",
    "        \n",
    "        # Save data as text file\n",
    "        all_data = np.array([all_mean_rewards, all_actor_loss]).astype(float)\n",
    "        all_data = all_data.T\n",
    "        with open(f'Data/Data_sub_{self.subpolicy}.txt', 'w') as file:\n",
    "            file.write('Rewards \\tActor Loss\\n')  # Write column headers\n",
    "            np.savetxt(file, all_data, delimiter='\\t', fmt='%f')\n",
    "        \n",
    "        \n",
    "    def validate_train(self):\n",
    "        \"\"\"\n",
    "        - Validate the agent during training in every epoch\n",
    "        \"\"\"\n",
    "        total_rewards = 0\n",
    "        actor = self.agent.actor.eval()\n",
    "        for _ in range(self.n_eval_episodes):\n",
    "            episode_reward = 0\n",
    "            seq_observation, seq_action = self.initial_window_all()\n",
    "            seq_observation_ = copy.deepcopy(seq_observation)\n",
    "            for t in range(self.episode_len):\n",
    "                action = self.sample_actions(seq_observation, seq_action, actor, reparameterize=False)\n",
    "                observation_, reward, done, info = self.env.step(action, self.subpolicy, self.static)\n",
    "                if info: print('--------------- Succesful ---------------')\n",
    "                episode_reward += reward\n",
    "                seq_observation_, seq_action = self.update_sequences(seq_observation_, seq_action, observation_, action)\n",
    "                seq_observation = seq_observation_\n",
    "                if done: break\n",
    "            total_rewards += episode_reward\n",
    "        return total_rewards / self.n_eval_episodes\n",
    "            \n",
    "        \n",
    "    def visualize_results(self):\n",
    "        \"\"\"\n",
    "        - Generate figures from the training\n",
    "        \"\"\"\n",
    "        plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "        df = pd.read_csv(f'Data/Data_sub_{self.subpolicy}.txt', delimiter='\\t')\n",
    "\n",
    "        for i, column in enumerate(df.columns):\n",
    "            column_data = df[column].values\n",
    "            means = pd.Series(column_data).rolling(window = 8).mean().values\n",
    "            stds = pd.Series(column_data).rolling(window = 8).std().values\n",
    "\n",
    "            plt.figure(figsize=(10,5))\n",
    "            plt.plot(means, label='Mean')\n",
    "            plt.fill_between(range(len(means)), means-stds, means+stds, alpha=0.2, label='Standard Deviation')\n",
    "            plt.legend()\n",
    "            plt.title(f'Sub-policy {self.subpolicy}', size=18) \n",
    "            plt.xlabel('Epochs', size=15)\n",
    "            plt.ylabel(f'{column}', size=15)\n",
    "            plt.savefig(f'Figures/Figures_sub_{self.subpolicy}/{column}.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "            \n",
    "    def test_actor_video(self, subpolicy = None):\n",
    "        # extract the image from the robot \n",
    "        # store images in the frames\n",
    "        # will need another list to store the states, use states as a label for the frames. then I'm going to use the states for labelling\n",
    "        # states = seq_observation\n",
    "        # familiarise with the fetch environment - what are the states, what are the actions, what are the rewards\n",
    "        # contact him on the email he sent me \n",
    "         \n",
    "        if subpolicy == None: subpolicy = self.subpolicy \n",
    "        trained_actor = self.get_trained_actors(self.subpolicy + 1)[-1] # get the agent of interest\n",
    "        frames = [] # store the frames for the video\n",
    "        \n",
    "        for _ in range(self.n_test_episodes):\n",
    "            seq_observation, seq_action = self.initial_window_all(subpolicy = subpolicy, train=False)\n",
    "            seq_observation_ = copy.deepcopy(seq_observation)\n",
    "            rewards = 0\n",
    "            for t in range(self.episode_len):  \n",
    "                action = self.sample_actions(seq_observation, seq_action, reparameterize=False)\n",
    "                    \n",
    "                view1, view2 = self.env.my_render() # two different views - front and left. Two images instead of 1, test 1 vs 2 views\n",
    "                # whether 1 is enough or 2 for the same state is better/enhances the training process\n",
    "                concatenated_image = np.concatenate((view1, view2), axis=1) # concatenate the two images\n",
    "                frames.append(concatenated_image)\n",
    "                observation_, reward, done, succesful = self.env.step(action, self.subpolicy, self.static)\n",
    "                rewards += reward\n",
    "                seq_observation_, seq_action = self.update_sequences(seq_observation_, seq_action, observation_, action)\n",
    "                seq_observation = seq_observation_\n",
    "                if succesful: print('--- Hell Yeah ---')\n",
    "                if done: break\n",
    "            print(rewards)\n",
    "\n",
    "        writer = cv2.VideoWriter(f'Videos/Subpolicy_{self.subpolicy}.mp4', cv2.VideoWriter_fourcc('m', 'p', '4', 'v'), 25, (frames[0].shape[1], frames[0].shape[0]))\n",
    "        for frame in frames:\n",
    "            writer.write(frame)\n",
    "        writer.release()\n",
    "        return frames\n",
    "        \n",
    "    def train_high_policy(self): \n",
    "        sub_actor_1, sub_actor_2,sub_actor_3 = self.get_trained_actors()[:]\n",
    "        \n",
    "        frames_all = []\n",
    "        labels_all = []\n",
    "        object_coorditanes = []\n",
    "        subpolicy = 1\n",
    "        for _ in range(10):#self.n_test_episodes):\n",
    "            frames = []\n",
    "            labels = []\n",
    "            seq_observation, _ , seq_action = self.initial_window_1() \n",
    "            seq_observation_ = copy.deepcopy(seq_observation)\n",
    "            rewards = 0\n",
    "            completed = False\n",
    "            sub_actor = sub_actor_1\n",
    "            for t in range(self.episode_len):\n",
    "                action = self.sample_actions(seq_observation, seq_action, reparameterize=False)\n",
    "                view1, view2 = self.env.my_render()\n",
    "                concatenated_image = np.concatenate((view1, view2), axis=1)\n",
    "                frames.append(concatenated_image)\n",
    "                \n",
    "                observation_, reward, done, succesful = self.env.step(action, subpolicy, self.static)\n",
    "                rewards += reward\n",
    "                seq_observation_, seq_action = self.update_sequences(seq_observation_, seq_action, observation_, action)\n",
    "                seq_observation = seq_observation_\n",
    "                \n",
    "                if succesful: \n",
    "                    if subpolicy == 1:\n",
    "                        labels.append(1)\n",
    "                        subpolicy = 2\n",
    "                        sub_actor = sub_actor_2\n",
    "                    elif subpolicy == 2:\n",
    "                        labels.append(2)\n",
    "                        subpolicy = 3\n",
    "                        sub_actor = sub_actor_3\n",
    "                    elif subpolicy == 3:\n",
    "                        labels.append(3)\n",
    "                        frames_all.append(frames)\n",
    "                        labels_all.append(labels)\n",
    "                        subpolicy = 1\n",
    "                        break\n",
    "                else: \n",
    "                    labels.append(0)\n",
    "                    if done: \n",
    "                        subpolicy = 1\n",
    "                        break\n",
    "            print(rewards)\n",
    "                \n",
    "        frames_all = sum(frames_all, [])\n",
    "        labels_all = sum(labels_all, [])\n",
    "        writer = cv2.VideoWriter(f'Videos/Complete_task.mp4', cv2.VideoWriter_fourcc('m', 'p', '4', 'v'), 25, (frames_all[0].shape[1], frames_all[0].shape[0]))\n",
    "        for frame in frames_all:\n",
    "            writer.write(frame)\n",
    "        writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fa8a3a4-4a7d-4326-a48a-aeff06914234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:39<00:00, 25.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Hey Fella, The Initial Exploration Has Just Finished -----------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m m \u001b[38;5;241m=\u001b[39m Agent_Training(subpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, image_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 153\u001b[0m, in \u001b[0;36mAgent_Training.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# ----------- Store Transitions --------------\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mremember(seq_observation[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], seq_action[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], reward, seq_observation_[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], done) \u001b[38;5;66;03m#######  CHANGE HERE FOR BUFFER!!!\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m seq_observation \u001b[38;5;241m=\u001b[39m seq_observation_\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done: \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/users/glori/documents/bioeng_yr4/yr4_project/ResponseWeightedAnalysis/fetch_cnn/SAC/Agent.py:92\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mmean(log_probs \u001b[38;5;241m*\u001b[39m alpha \u001b[38;5;241m-\u001b[39m T\u001b[38;5;241m.\u001b[39mmin(q1_new_policy, q2_new_policy)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 92\u001b[0m \u001b[43mactor_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m## Q VALUE UPDATE --------------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fetchLinux/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/fetchLinux/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m = Agent_Training(subpolicy = 1, image_dims = 64)\n",
    "m.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
