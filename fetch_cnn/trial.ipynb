{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a80c1ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from SAC.Agent import Agent\n",
    "from Robot.Environment import Env\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow, show   \n",
    "from matplotlib import rcParams\n",
    "from IPython import display\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import cv2\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02a95870-25e7-4005-86d6-0bf3454cfe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_Training():\n",
    "    def __init__(self, subpolicy, image_dims, seed = 1):\n",
    "        self.seed = seed\n",
    "        self.image_dims = image_dims\n",
    "        self.subpolicy = subpolicy\n",
    "        self.static = False\n",
    "        self.n_epochs = 250\n",
    "        self.n_eval_episodes = 5\n",
    "        self.n_test_episodes = 15\n",
    "        self.episode_len = 100\n",
    "        self.exploration_steps = 1000\n",
    "        self.seq_len = 4\n",
    "        \n",
    "        self.env = Env(self.image_dims, self.seed)\n",
    "        self.n_actions = self.env.action_space.shape[0]\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.agent = Agent(self.image_dims, self.n_actions, self.seq_len, self.env, self.seed) \n",
    "            \n",
    "        if self.subpolicy > 1: self.trained_actors = self.get_trained_actors(target_subpolicy = self.subpolicy)\n",
    "            \n",
    "    \"\"\"\n",
    "    REMEMBER - WE PROCESS THE IMAGES INSIDE THE ENV CLASS, WHERE WE CONCATENATE THE TWO IMAGES (FRONT AND LEFT CAMERA) \n",
    "    AS 6 CHANNELS, RESHAPE TO (CHANNEL, HEIGHT, WEIDTH), AND THEN NORMALIZE (IMAGES / 255) - see: def my_render(image_dims)\n",
    "    # already done in the environment class\n",
    "    \"\"\"\n",
    "            \n",
    "    #### function to \n",
    "    def integrate_weights(self):\n",
    "        \"\"\"\n",
    "        - Specify which layers to transfer, and which to freeze\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_tensor(self, observation):\n",
    "        observation = torch.Tensor(observation).unsqueeze(0).to(self.device)\n",
    "        return observation\n",
    "    \n",
    "    def get_trained_actors(self, target_subpolicy):\n",
    "        \"\"\"\n",
    "        - Get the trained actor either to test, or to get to the initial state of the next subpolicy\n",
    "        \"\"\"\n",
    "        trained_actors = []\n",
    "        for i in range(1, target_subpolicy):\n",
    "            new_agent = Agent(self.image_dims, self.n_actions, self.seq_len, self.env, self.seed) \n",
    "            trained_actor = new_agent.actor\n",
    "            trained_actor.load_state_dict(torch.load(f'Models/models_sub_{i}/Actor'))\n",
    "            trained_actor.eval()\n",
    "            trained_actors.append(trained_actor)\n",
    "        return trained_actors\n",
    "    \n",
    "    def update_sequences(self, seq_observation, seq_action, observation, actions):\n",
    "        seq_observation = np.roll(seq_observation, -1, axis=0)\n",
    "        seq_observation[-1] = observation\n",
    "        seq_action = np.roll(seq_action, -1, axis=0)\n",
    "        seq_action[-1] = actions\n",
    "        return seq_observation, seq_action\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample_actions(self, seq_observation, seq_action, actor, reparameterize):\n",
    "        action, _ = actor.sample_normal(self.get_tensor(seq_observation), self.get_tensor(seq_action[1:]), reparameterize=reparameterize)\n",
    "        action = action.cpu().detach().numpy()[0]\n",
    "        return action\n",
    "    \n",
    "    def initial_window(self):\n",
    "        \"\"\"\n",
    "        - Get the initial observation sequence for first subpolicy\n",
    "        - We use neutral actions (0) to minimize the interactions with env\n",
    "        \"\"\"\n",
    "        observation = self.env.reset()\n",
    "        seq_observation = []\n",
    "        seq_observation_ = []\n",
    "        seq_actions = []        \n",
    "        for i in range(self.seq_len): \n",
    "            subpolicy = 1\n",
    "            action = [0 for i in range(self.n_actions)]\n",
    "            observation_, _, done, _ = self.env.step(action, subpolicy, self.static)\n",
    "            seq_observation.append(observation)\n",
    "            seq_observation_.append(observation_)\n",
    "            seq_actions.append(action)\n",
    "            observation = observation_\n",
    "        return np.array(seq_observation), np.array(seq_observation_), np.array(seq_actions, dtype=np.float64)\n",
    "    \n",
    "\n",
    "    def initial_window_all(self, subpolicy=None, train=False):\n",
    "        \"\"\"\n",
    "        - Get the initial observation sequence for all subpolicy\n",
    "        \"\"\"\n",
    "        if subpolicy is None: subpolicy = self.subpolicy\n",
    "\n",
    "        if subpolicy == 1:\n",
    "            final_seq_observation, final_seq_observation_, final_seq_actions = self.initial_window()\n",
    "        else:\n",
    "            succesfull = False\n",
    "            for trained_subpolicy in range(1, subpolicy):\n",
    "                trained_actor = self.trained_actors[trained_subpolicy - 1]            \n",
    "                while not succesfull:\n",
    "                    if trained_subpolicy == 1: seq_observation, _, seq_actions = self.initial_window()\n",
    "                    seq_observation_ = copy.deepcopy(seq_observation)\n",
    "                    last_seq_observation = []\n",
    "                    last_seq_observation_ = []\n",
    "                    last_seq_actions = []\n",
    "                    for t in range(self.episode_len):\n",
    "                        action = self.sample_actions(seq_observation, seq_actions, trained_actor, reparameterize=False)\n",
    "                        observation_, _, done, succesfull = self.env.step(action, trained_subpolicy, self.static)\n",
    "                        seq_observation_, seq_actions = self.update_sequences(seq_observation_, seq_actions, observation_, action)\n",
    "                        last_seq_observation.append(seq_observation)\n",
    "                        last_seq_observation_.append(seq_observation_)\n",
    "                        last_seq_actions.append(seq_actions)\n",
    "                        seq_observation = seq_observation_\n",
    "                        if done: break\n",
    "            final_seq_observation ,final_seq_observation_ , final_seq_actions = last_seq_observation[-1], last_seq_observation_[-1], last_seq_actions[-1]\n",
    "        if train:\n",
    "            for s in range(0, self.seq_len - 1):\n",
    "                self.agent.remember(final_seq_observation[s], final_seq_actions[s], None, final_seq_observation_[s], 0)\n",
    "            \n",
    "        return final_seq_observation, final_seq_actions\n",
    "\n",
    "    \n",
    "    def initial_exploration(self):\n",
    "        \"\"\"\n",
    "        - Explore the environomnt to fill in the buffer and exceed the batch size\n",
    "        \"\"\"\n",
    "        seq_observation, seq_action = self.initial_window_all()        \n",
    "        seq_observation_ = copy.deepcopy(seq_observation)\n",
    "        for t in tqdm(range(self.exploration_steps)):\n",
    "            action = self.sample_actions(seq_observation, seq_action, self.agent.actor, reparameterize=True)\n",
    "            observation_, reward, done, _ = self.env.step(action, self.subpolicy, self.static)\n",
    "            seq_observation_, seq_action = self.update_sequences(seq_observation_, seq_action, observation_, action)\n",
    "            self.agent.remember(seq_observation[-1], seq_action[-1], reward, seq_observation_[-1], done)\n",
    "            seq_observation = seq_observation_\n",
    "            if done: \n",
    "                seq_observation, seq_action = self.initial_window_all()   \n",
    "                seq_observation_ = copy.deepcopy(seq_observation)\n",
    "        print('------------ Hey Fella, The Initial Exploration Has Just Finished -----------------')\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        - Train the agent, save metrics for visualization, and save model\n",
    "        \"\"\"\n",
    "        all_mean_rewards, all_actor_loss = [], []\n",
    "        self.initial_exploration() \n",
    "        with tqdm(total = self.n_epochs) as pbar:\n",
    "            for epoch in range(self.n_epochs):\n",
    "                seq_observation, seq_action = self.initial_window_all()  \n",
    "                seq_observation_ = copy.deepcopy(seq_observation)\n",
    "                for t in range(self.episode_len):\n",
    "                    action = self.sample_actions(seq_observation, seq_action, self.agent.actor,  reparameterize=True)\n",
    "                    observation_, reward, done, info = self.env.step(action, self.subpolicy, self.static) \n",
    "                    seq_observation_, seq_action = self.update_sequences(seq_observation_, seq_action, observation_, action)\n",
    "                    # ----------- Store Transitions --------------\n",
    "                    self.agent.remember(seq_observation[-1], seq_action[-1], reward, seq_observation_[-1], done) #######  CHANGE HERE FOR BUFFER!!!\n",
    "                    actor_loss = self.agent.learn()\n",
    "                    seq_observation = seq_observation_\n",
    "                    if done: break\n",
    "\n",
    "                mean_rewards = self.validate_train()\n",
    "                self.agent.save_models(self.subpolicy, epoch)\n",
    "                all_mean_rewards.append(mean_rewards)\n",
    "                all_actor_loss.append(actor_loss)\n",
    "                print(f'Epoch: {epoch}, Rewards: {mean_rewards}, Actor Loss: {actor_loss}')\n",
    "\n",
    "                plt.plot(all_mean_rewards)\n",
    "                plt.show()\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "                time.sleep(0.2)\n",
    "                pbar.update(1)\n",
    "        plt.plot(all_mean_rewards)\n",
    "        \n",
    "        # Save data as text file\n",
    "        all_data = np.array([all_mean_rewards, all_actor_loss]).astype(float)\n",
    "        all_data = all_data.T\n",
    "        with open(f'Data/Data_sub_{self.subpolicy}.txt', 'w') as file:\n",
    "            file.write('Rewards \\tActor Loss\\n')  # Write column headers\n",
    "            np.savetxt(file, all_data, delimiter='\\t', fmt='%f')\n",
    "        \n",
    "        \n",
    "    def validate_train(self):\n",
    "        \"\"\"\n",
    "        - Validate the agent during training in every epoch\n",
    "        \"\"\"\n",
    "        total_rewards = 0\n",
    "        actor = self.agent.actor.eval()\n",
    "        for _ in range(self.n_eval_episodes):\n",
    "            episode_reward = 0\n",
    "            seq_observation, seq_action = self.initial_window_all()\n",
    "            seq_observation_ = copy.deepcopy(seq_observation)\n",
    "            for t in range(self.episode_len):\n",
    "                action = self.sample_actions(seq_observation, seq_action, actor, reparameterize=False)\n",
    "                observation_, reward, done, info = self.env.step(action, self.subpolicy, self.static)\n",
    "                if info: print('--------------- Succesful ---------------')\n",
    "                episode_reward += reward\n",
    "                seq_observation_, seq_action = self.update_sequences(seq_observation_, seq_action, observation_, action)\n",
    "                seq_observation = seq_observation_\n",
    "                if done: break\n",
    "            total_rewards += episode_reward\n",
    "        return total_rewards / self.n_eval_episodes\n",
    "            \n",
    "        \n",
    "    def visualize_results(self):\n",
    "        \"\"\"\n",
    "        - Generate figures from the training\n",
    "        \"\"\"\n",
    "        plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "        df = pd.read_csv(f'Data/Data_sub_{self.subpolicy}.txt', delimiter='\\t')\n",
    "\n",
    "        for i, column in enumerate(df.columns):\n",
    "            column_data = df[column].values\n",
    "            means = pd.Series(column_data).rolling(window = 8).mean().values\n",
    "            stds = pd.Series(column_data).rolling(window = 8).std().values\n",
    "\n",
    "            plt.figure(figsize=(10,5))\n",
    "            plt.plot(means, label='Mean')\n",
    "            plt.fill_between(range(len(means)), means-stds, means+stds, alpha=0.2, label='Standard Deviation')\n",
    "            plt.legend()\n",
    "            plt.title(f'Sub-policy {self.subpolicy}', size=18) \n",
    "            plt.xlabel('Epochs', size=15)\n",
    "            plt.ylabel(f'{column}', size=15)\n",
    "            plt.savefig(f'Figures/Figures_sub_{self.subpolicy}/{column}.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "            \n",
    "    def test_actor_video(self, subpolicy = None):\n",
    "        # extract the image from the robot \n",
    "        # store images in the frames\n",
    "        # will need another list to store the states, use states as a label for the frames. then I'm going to use the states for labelling\n",
    "        # states = seq_observation\n",
    "        # familiarise with the fetch environment - what are the states, what are the actions, what are the rewards\n",
    "        # contact him on the email he sent me \n",
    "         \n",
    "        if subpolicy == None: subpolicy = self.subpolicy \n",
    "        # trained_actor = self.get_trained_actors(self.subpolicy + 1)[-1] # get the agent of interest\n",
    "        frames = [] # store the frames for the video\n",
    "        \n",
    "        for i in range(self.n_test_episodes):\n",
    "            seq_observation, seq_action = self.initial_window_all(subpolicy = subpolicy, train=False)\n",
    "            seq_observation_ = copy.deepcopy(seq_observation)\n",
    "            rewards = 0\n",
    "            for t in range(self.episode_len):  \n",
    "                # action = self.sample_actions(seq_observation, seq_action, reparameterize=False)\n",
    "                # action = self.env.action_space.sample()\n",
    "                action = [0., 0., 0.]\n",
    "                view1, view2 = self.env.my_render(dims = self.image_dims) # two different views - front and left. Two images instead of 1, test 1 vs 2 views\n",
    "                # whether 1 is enough or 2 for the same state is better/enhances the training process\n",
    "                # concatenated_image = np.concatenate((view1, view2), axis=1) # concatenate the two images\n",
    "                concatenated_image = np.concatenate([view1[:,:,::-1], view2[:,:,::-1]], axis=2).transpose(2, 1, 0) / 255.0 \n",
    "                frames.append(concatenated_image)\n",
    "                observation_, reward, done, succesful = self.env.step(action, self.subpolicy, static=False)\n",
    "                rewards += reward\n",
    "                seq_observation_, seq_action = self.update_sequences(seq_observation_, seq_action, observation_, action)\n",
    "                seq_observation = seq_observation_\n",
    "                if succesful: print('--- Hell Yeah ---')\n",
    "                if done:\n",
    "                    print(\"done\")\n",
    "                    break\n",
    "            print(rewards)\n",
    "            print(i)\n",
    "\n",
    "        writer = cv2.VideoWriter(f'Videos/Subpolicy_{self.subpolicy}.mp4', cv2.VideoWriter_fourcc('m', 'p', '4', 'v'), 25, (frames[0].shape[1], frames[0].shape[0]))\n",
    "        for frame in frames:\n",
    "            writer.write(frame)\n",
    "        writer.release()\n",
    "\n",
    "        return frames\n",
    "        \n",
    "    def train_high_policy(self): \n",
    "        sub_actor_1, sub_actor_2,sub_actor_3 = self.get_trained_actors()[:]\n",
    "        \n",
    "        frames_all = []\n",
    "        labels_all = []\n",
    "        object_coorditanes = []\n",
    "        subpolicy = 1\n",
    "        for _ in range(10):#self.n_test_episodes):\n",
    "            frames = []\n",
    "            labels = []\n",
    "            seq_observation, _ , seq_action = self.initial_window_1() \n",
    "            seq_observation_ = copy.deepcopy(seq_observation)\n",
    "            rewards = 0\n",
    "            completed = False\n",
    "            sub_actor = sub_actor_1\n",
    "            for t in range(self.episode_len):\n",
    "                action = self.sample_actions(seq_observation, seq_action, reparameterize=False)\n",
    "                view1, view2 = self.env.my_render()\n",
    "                concatenated_image = np.concatenate((view1, view2), axis=1)\n",
    "                frames.append(concatenated_image)\n",
    "                \n",
    "                observation_, reward, done, succesful = self.env.step(action, subpolicy, self.static)\n",
    "                rewards += reward\n",
    "                seq_observation_, seq_action = self.update_sequences(seq_observation_, seq_action, observation_, action)\n",
    "                seq_observation = seq_observation_\n",
    "                \n",
    "                if succesful: \n",
    "                    if subpolicy == 1:\n",
    "                        labels.append(1)\n",
    "                        subpolicy = 2\n",
    "                        sub_actor = sub_actor_2\n",
    "                    elif subpolicy == 2:\n",
    "                        labels.append(2)\n",
    "                        subpolicy = 3\n",
    "                        sub_actor = sub_actor_3\n",
    "                    elif subpolicy == 3:\n",
    "                        labels.append(3)\n",
    "                        frames_all.append(frames)\n",
    "                        labels_all.append(labels)\n",
    "                        subpolicy = 1\n",
    "                        break\n",
    "                else: \n",
    "                    labels.append(0)\n",
    "                    if done: \n",
    "                        subpolicy = 1\n",
    "                        break\n",
    "            print(rewards)\n",
    "                \n",
    "        frames_all = sum(frames_all, [])\n",
    "        labels_all = sum(labels_all, [])\n",
    "        writer = cv2.VideoWriter(f'Videos/Complete_task.mp4', cv2.VideoWriter_fourcc('m', 'p', '4', 'v'), 25, (frames_all[0].shape[1], frames_all[0].shape[0]))\n",
    "        for frame in frames_all:\n",
    "            writer.write(frame)\n",
    "        writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa8a3a4-4a7d-4326-a48a-aeff06914234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-21.927916734897952\n",
      "0\n",
      "-29.248176110621262\n",
      "1\n",
      "-26.99283435081768\n",
      "2\n",
      "-25.09027284370688\n",
      "3\n",
      "-21.81251808299739\n",
      "4\n",
      "-25.237218269252907\n",
      "5\n",
      "-29.220102271872207\n",
      "6\n",
      "-27.774168031387166\n",
      "7\n",
      "-21.361416917600497\n",
      "8\n",
      "-30.227836759520017\n",
      "9\n",
      "-26.57450158609342\n",
      "10\n",
      "-22.04503775213275\n",
      "11\n",
      "-20.998650806930204\n",
      "12\n",
      "-20.430052156480336\n",
      "13\n",
      "-21.824561183795083\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "subpolicy = 1\n",
    "self = Agent_Training(subpolicy = 1, image_dims = 64)\n",
    "frames = self.test_actor_video(subpolicy = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3e2247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "side_images = []\n",
    "for i in range(len(frames)):\n",
    "    x = frames[i]\n",
    "    # swap the first and last dimensions\n",
    "    x = np.swapaxes(x, 0, 2)\n",
    "    side_images.append(x[:, :, 3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc6d1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image in side_images:\n",
    "    plt.figure(figsize=(9, 9))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off the axis\n",
    "    plt.pause(0.005)  # Pause for 1 second between images\n",
    "    # clear the current image\n",
    "    clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
