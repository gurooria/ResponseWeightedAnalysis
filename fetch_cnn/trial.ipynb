{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a80c1ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from SAC.Agent import Agent\n",
    "from Robot.Environment import Env\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow, show   \n",
    "from matplotlib import rcParams\n",
    "from IPython import display\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import cv2\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "02a95870-25e7-4005-86d6-0bf3454cfe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_Training():\n",
    "    def __init__(self, subpolicy, image_dims, seed = 1):\n",
    "        self.seed = seed\n",
    "        self.image_dims = image_dims\n",
    "        self.subpolicy = subpolicy\n",
    "        self.static = False\n",
    "        self.n_epochs = 250\n",
    "        self.n_eval_episodes = 5\n",
    "        self.n_test_episodes = 15\n",
    "        self.episode_len = 100\n",
    "        self.exploration_steps = 1000\n",
    "        self.seq_len = 4\n",
    "        \n",
    "        self.env = Env(self.image_dims, self.seed)\n",
    "        self.n_actions = self.env.action_space.shape[0]\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.agent = Agent(self.image_dims, self.n_actions, self.seq_len, self.env, self.seed) \n",
    "            \n",
    "        if self.subpolicy > 1: self.trained_actors = self.get_trained_actors(target_subpolicy = self.subpolicy)\n",
    "            \n",
    "    \"\"\"\n",
    "    REMEMBER - WE PROCESS THE IMAGES INSIDE THE ENV CLASS, WHERE WE CONCATENATE THE TWO IMAGES (FRONT AND LEFT CAMERA) \n",
    "    AS 6 CHANNELS, RESHAPE TO (CHANNEL, HEIGHT, WEIDTH), AND THEN NORMALIZE (IMAGES / 255) - see: def my_render(image_dims)\n",
    "    # already done in the environment class\n",
    "    \"\"\"\n",
    "            \n",
    "    #### function to \n",
    "    def integrate_weights(self):\n",
    "        \"\"\"\n",
    "        - Specify which layers to transfer, and which to freeze\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_tensor(self, observation):\n",
    "        observation = torch.Tensor(observation).unsqueeze(0).to(self.device)\n",
    "        return observation\n",
    "    \n",
    "    def get_trained_actors(self, target_subpolicy):\n",
    "        \"\"\"\n",
    "        - Get the trained actor either to test, or to get to the initial state of the next subpolicy\n",
    "        \"\"\"\n",
    "        trained_actors = []\n",
    "        for i in range(1, target_subpolicy):\n",
    "            new_agent = Agent(self.image_dims, self.n_actions, self.seq_len, self.env, self.seed) \n",
    "            trained_actor = new_agent.actor\n",
    "            trained_actor.load_state_dict(torch.load(f'Models/models_sub_{i}/Actor'))\n",
    "            trained_actor.eval()\n",
    "            trained_actors.append(trained_actor)\n",
    "        return trained_actors\n",
    "    \n",
    "    def update_sequences(self, seq_observation, seq_action, observation, actions):\n",
    "        seq_observation = np.roll(seq_observation, -1, axis=0)\n",
    "        seq_observation[-1] = observation\n",
    "        seq_action = np.roll(seq_action, -1, axis=0)\n",
    "        seq_action[-1] = actions\n",
    "        return seq_observation, seq_action\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample_actions(self, seq_observation, seq_action, actor, reparameterize):\n",
    "        action, _ = actor.sample_normal(self.get_tensor(seq_observation), self.get_tensor(seq_action[1:]), reparameterize=reparameterize)\n",
    "        action = action.cpu().detach().numpy()[0]\n",
    "        return action\n",
    "    \n",
    "    def initial_window(self):\n",
    "        \"\"\"\n",
    "        - Get the initial observation sequence for first subpolicy\n",
    "        - We use neutral actions (0) to minimize the interactions with env\n",
    "        \"\"\"\n",
    "        observation = self.env.reset()\n",
    "        seq_observation = []\n",
    "        seq_observation_ = []\n",
    "        seq_actions = []        \n",
    "        for i in range(self.seq_len): \n",
    "            subpolicy = 1\n",
    "            action = [0 for i in range(self.n_actions)]\n",
    "            observation_, _, done, _ = self.env.step(action, subpolicy, self.static)\n",
    "            seq_observation.append(observation)\n",
    "            seq_observation_.append(observation_)\n",
    "            seq_actions.append(action)\n",
    "            observation = observation_\n",
    "        return np.array(seq_observation), np.array(seq_observation_), np.array(seq_actions, dtype=np.float64)\n",
    "    \n",
    "\n",
    "    def initial_window_all(self, subpolicy=None, train=False):\n",
    "        \"\"\"\n",
    "        - Get the initial observation sequence for all subpolicy\n",
    "        \"\"\"\n",
    "        if subpolicy is None: subpolicy = self.subpolicy\n",
    "\n",
    "        if subpolicy == 1:\n",
    "            final_seq_observation, final_seq_observation_, final_seq_actions = self.initial_window()\n",
    "        else:\n",
    "            succesfull = False\n",
    "            for trained_subpolicy in range(1, subpolicy):\n",
    "                trained_actor = self.trained_actors[trained_subpolicy - 1]            \n",
    "                while not succesfull:\n",
    "                    if trained_subpolicy == 1: seq_observation, _, seq_actions = self.initial_window()\n",
    "                    seq_observation_ = copy.deepcopy(seq_observation)\n",
    "                    last_seq_observation = []\n",
    "                    last_seq_observation_ = []\n",
    "                    last_seq_actions = []\n",
    "                    for t in range(self.episode_len):\n",
    "                        action = self.sample_actions(seq_observation, seq_actions, trained_actor, reparameterize=False)\n",
    "                        observation_, _, done, succesfull = self.env.step(action, trained_subpolicy, self.static)\n",
    "                        seq_observation_, seq_actions = self.update_sequences(seq_observation_, seq_actions, observation_, action)\n",
    "                        last_seq_observation.append(seq_observation)\n",
    "                        last_seq_observation_.append(seq_observation_)\n",
    "                        last_seq_actions.append(seq_actions)\n",
    "                        seq_observation = seq_observation_\n",
    "                        if done: break\n",
    "            final_seq_observation ,final_seq_observation_ , final_seq_actions = last_seq_observation[-1], last_seq_observation_[-1], last_seq_actions[-1]\n",
    "        if train:\n",
    "            for s in range(0, self.seq_len - 1):\n",
    "                self.agent.remember(final_seq_observation[s], final_seq_actions[s], None, final_seq_observation_[s], 0)\n",
    "            \n",
    "        return final_seq_observation, final_seq_actions\n",
    "\n",
    "    \n",
    "    def initial_exploration(self):\n",
    "        \"\"\"\n",
    "        - Explore the environomnt to fill in the buffer and exceed the batch size\n",
    "        \"\"\"\n",
    "        seq_observation, seq_action = self.initial_window_all()        \n",
    "        seq_observation_ = copy.deepcopy(seq_observation)\n",
    "        for t in tqdm(range(self.exploration_steps)):\n",
    "            action = self.sample_actions(seq_observation, seq_action, self.agent.actor, reparameterize=True)\n",
    "            observation_, reward, done, _ = self.env.step(action, self.subpolicy, self.static)\n",
    "            seq_observation_, seq_action = self.update_sequences(seq_observation_, seq_action, observation_, action)\n",
    "            self.agent.remember(seq_observation[-1], seq_action[-1], reward, seq_observation_[-1], done)\n",
    "            seq_observation = seq_observation_\n",
    "            if done: \n",
    "                seq_observation, seq_action = self.initial_window_all()   \n",
    "                seq_observation_ = copy.deepcopy(seq_observation)\n",
    "        print('------------ Hey Fella, The Initial Exploration Has Just Finished -----------------')\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        - Train the agent, save metrics for visualization, and save model\n",
    "        \"\"\"\n",
    "        all_mean_rewards, all_actor_loss = [], []\n",
    "        self.initial_exploration() \n",
    "        with tqdm(total = self.n_epochs) as pbar:\n",
    "            for epoch in range(self.n_epochs):\n",
    "                seq_observation, seq_action = self.initial_window_all()  \n",
    "                seq_observation_ = copy.deepcopy(seq_observation)\n",
    "                for t in range(self.episode_len):\n",
    "                    action = self.sample_actions(seq_observation, seq_action, self.agent.actor,  reparameterize=True)\n",
    "                    observation_, reward, done, info = self.env.step(action, self.subpolicy, self.static) \n",
    "                    seq_observation_, seq_action = self.update_sequences(seq_observation_, seq_action, observation_, action)\n",
    "                    # ----------- Store Transitions --------------\n",
    "                    self.agent.remember(seq_observation[-1], seq_action[-1], reward, seq_observation_[-1], done) #######  CHANGE HERE FOR BUFFER!!!\n",
    "                    actor_loss = self.agent.learn()\n",
    "                    seq_observation = seq_observation_\n",
    "                    if done: break\n",
    "\n",
    "                mean_rewards = self.validate_train()\n",
    "                self.agent.save_models(self.subpolicy, epoch)\n",
    "                all_mean_rewards.append(mean_rewards)\n",
    "                all_actor_loss.append(actor_loss)\n",
    "                print(f'Epoch: {epoch}, Rewards: {mean_rewards}, Actor Loss: {actor_loss}')\n",
    "\n",
    "                plt.plot(all_mean_rewards)\n",
    "                plt.show()\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "                time.sleep(0.2)\n",
    "                pbar.update(1)\n",
    "        plt.plot(all_mean_rewards)\n",
    "        \n",
    "        # Save data as text file\n",
    "        all_data = np.array([all_mean_rewards, all_actor_loss]).astype(float)\n",
    "        all_data = all_data.T\n",
    "        with open(f'Data/Data_sub_{self.subpolicy}.txt', 'w') as file:\n",
    "            file.write('Rewards \\tActor Loss\\n')  # Write column headers\n",
    "            np.savetxt(file, all_data, delimiter='\\t', fmt='%f')\n",
    "        \n",
    "        \n",
    "    def validate_train(self):\n",
    "        \"\"\"\n",
    "        - Validate the agent during training in every epoch\n",
    "        \"\"\"\n",
    "        total_rewards = 0\n",
    "        actor = self.agent.actor.eval()\n",
    "        for _ in range(self.n_eval_episodes):\n",
    "            episode_reward = 0\n",
    "            seq_observation, seq_action = self.initial_window_all()\n",
    "            seq_observation_ = copy.deepcopy(seq_observation)\n",
    "            for t in range(self.episode_len):\n",
    "                action = self.sample_actions(seq_observation, seq_action, actor, reparameterize=False)\n",
    "                observation_, reward, done, info = self.env.step(action, self.subpolicy, self.static)\n",
    "                if info: print('--------------- Succesful ---------------')\n",
    "                episode_reward += reward\n",
    "                seq_observation_, seq_action = self.update_sequences(seq_observation_, seq_action, observation_, action)\n",
    "                seq_observation = seq_observation_\n",
    "                if done: break\n",
    "            total_rewards += episode_reward\n",
    "        return total_rewards / self.n_eval_episodes\n",
    "            \n",
    "        \n",
    "    def visualize_results(self):\n",
    "        \"\"\"\n",
    "        - Generate figures from the training\n",
    "        \"\"\"\n",
    "        plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "        df = pd.read_csv(f'Data/Data_sub_{self.subpolicy}.txt', delimiter='\\t')\n",
    "\n",
    "        for i, column in enumerate(df.columns):\n",
    "            column_data = df[column].values\n",
    "            means = pd.Series(column_data).rolling(window = 8).mean().values\n",
    "            stds = pd.Series(column_data).rolling(window = 8).std().values\n",
    "\n",
    "            plt.figure(figsize=(10,5))\n",
    "            plt.plot(means, label='Mean')\n",
    "            plt.fill_between(range(len(means)), means-stds, means+stds, alpha=0.2, label='Standard Deviation')\n",
    "            plt.legend()\n",
    "            plt.title(f'Sub-policy {self.subpolicy}', size=18) \n",
    "            plt.xlabel('Epochs', size=15)\n",
    "            plt.ylabel(f'{column}', size=15)\n",
    "            plt.savefig(f'Figures/Figures_sub_{self.subpolicy}/{column}.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "            \n",
    "    def test_actor_video(self, subpolicy = None):\n",
    "        # extract the image from the robot \n",
    "        # store images in the frames\n",
    "        # will need another list to store the states, use states as a label for the frames. then I'm going to use the states for labelling\n",
    "        # states = seq_observation\n",
    "        # familiarise with the fetch environment - what are the states, what are the actions, what are the rewards\n",
    "        # contact him on the email he sent me \n",
    "         \n",
    "        if subpolicy == None: subpolicy = self.subpolicy \n",
    "        # trained_actor = self.get_trained_actors(self.subpolicy + 1)[-1] # get the agent of interest\n",
    "        frames = [] # store the frames for the video\n",
    "        \n",
    "        for i in range(self.n_test_episodes):\n",
    "            seq_observation, seq_action = self.initial_window_all(subpolicy = subpolicy, train=False)\n",
    "            seq_observation_ = copy.deepcopy(seq_observation)\n",
    "            rewards = 0\n",
    "            for t in range(self.episode_len):  \n",
    "                # action = self.sample_actions(seq_observation, seq_action, reparameterize=False)\n",
    "                action = self.env.action_space.sample()\n",
    "                # action = [0., 0., 0.]\n",
    "                view1, view2 = self.env.my_render(dims = self.image_dims) # two different views - front and left. Two images instead of 1, test 1 vs 2 views\n",
    "                # whether 1 is enough or 2 for the same state is better/enhances the training process\n",
    "                # concatenated_image = np.concatenate((view1, view2), axis=1) # concatenate the two images\n",
    "                concatenated_image = np.concatenate([view1[:,:,::-1], view2[:,:,::-1]], axis=2).transpose(2, 1, 0) / 255.0 \n",
    "                frames.append(concatenated_image)\n",
    "                observation_, reward, done, succesful = self.env.step(action, self.subpolicy, static=False)\n",
    "                rewards += reward\n",
    "                seq_observation_, seq_action = self.update_sequences(seq_observation_, seq_action, observation_, action)\n",
    "                seq_observation = seq_observation_\n",
    "                if succesful: print('--- Hell Yeah ---')\n",
    "                if done:\n",
    "                    print(\"done\")\n",
    "                    break\n",
    "            print(rewards)\n",
    "            print(i)\n",
    "\n",
    "        writer = cv2.VideoWriter(f'Videos/Subpolicy_{self.subpolicy}.mp4', cv2.VideoWriter_fourcc('m', 'p', '4', 'v'), 25, (frames[0].shape[1], frames[0].shape[0]))\n",
    "        for frame in frames:\n",
    "            writer.write(frame)\n",
    "        writer.release()\n",
    "\n",
    "        return frames\n",
    "        \n",
    "    def train_high_policy(self): \n",
    "        sub_actor_1, sub_actor_2,sub_actor_3 = self.get_trained_actors()[:]\n",
    "        \n",
    "        frames_all = []\n",
    "        labels_all = []\n",
    "        object_coorditanes = []\n",
    "        subpolicy = 1\n",
    "        for _ in range(10):#self.n_test_episodes):\n",
    "            frames = []\n",
    "            labels = []\n",
    "            seq_observation, _ , seq_action = self.initial_window_1() \n",
    "            seq_observation_ = copy.deepcopy(seq_observation)\n",
    "            rewards = 0\n",
    "            completed = False\n",
    "            sub_actor = sub_actor_1\n",
    "            for t in range(self.episode_len):\n",
    "                action = self.sample_actions(seq_observation, seq_action, reparameterize=False)\n",
    "                view1, view2 = self.env.my_render()\n",
    "                concatenated_image = np.concatenate((view1, view2), axis=1)\n",
    "                frames.append(concatenated_image)\n",
    "                \n",
    "                observation_, reward, done, succesful = self.env.step(action, subpolicy, self.static)\n",
    "                rewards += reward\n",
    "                seq_observation_, seq_action = self.update_sequences(seq_observation_, seq_action, observation_, action)\n",
    "                seq_observation = seq_observation_\n",
    "                \n",
    "                if succesful: \n",
    "                    if subpolicy == 1:\n",
    "                        labels.append(1)\n",
    "                        subpolicy = 2\n",
    "                        sub_actor = sub_actor_2\n",
    "                    elif subpolicy == 2:\n",
    "                        labels.append(2)\n",
    "                        subpolicy = 3\n",
    "                        sub_actor = sub_actor_3\n",
    "                    elif subpolicy == 3:\n",
    "                        labels.append(3)\n",
    "                        frames_all.append(frames)\n",
    "                        labels_all.append(labels)\n",
    "                        subpolicy = 1\n",
    "                        break\n",
    "                else: \n",
    "                    labels.append(0)\n",
    "                    if done: \n",
    "                        subpolicy = 1\n",
    "                        break\n",
    "            print(rewards)\n",
    "                \n",
    "        frames_all = sum(frames_all, [])\n",
    "        labels_all = sum(labels_all, [])\n",
    "        writer = cv2.VideoWriter(f'Videos/Complete_task.mp4', cv2.VideoWriter_fourcc('m', 'p', '4', 'v'), 25, (frames_all[0].shape[1], frames_all[0].shape[0]))\n",
    "        for frame in frames_all:\n",
    "            writer.write(frame)\n",
    "        writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5fa8a3a4-4a7d-4326-a48a-aeff06914234",
   "metadata": {},
   "outputs": [],
   "source": [
    "subpolicy = 1\n",
    "self = Agent_Training(subpolicy = 1, image_dims = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7d6e50ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-31.79519998441348\n",
      "0\n",
      "done\n",
      "-18.277655580506124\n",
      "1\n",
      "-28.45261658129008\n",
      "2\n",
      "-25.677969070641936\n",
      "3\n",
      "-16.59361381288178\n",
      "4\n",
      "-25.33990708510229\n",
      "5\n",
      "done\n",
      "-27.761438666896264\n",
      "6\n",
      "-26.529499938954967\n",
      "7\n",
      "-30.599410839437393\n",
      "8\n",
      "-26.698765187849148\n",
      "9\n",
      "-24.376943163536218\n",
      "10\n",
      "done\n",
      "-19.226079029328382\n",
      "11\n",
      "-30.821034463932197\n",
      "12\n",
      "done\n",
      "-16.9680850712015\n",
      "13\n",
      "-24.525216142263957\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "frames = self.test_actor_video(subpolicy = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d3e2247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "side_images = []\n",
    "for i in range(len(frames)):\n",
    "    x = frames[i]\n",
    "    # swap the first and last dimensions\n",
    "    x = np.swapaxes(x, 0, 2)\n",
    "    side_images.append(x[:, :, 3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cbc6d1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image in side_images:\n",
    "    plt.figure(figsize=(9, 9))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off the axis\n",
    "    plt.pause(0.005)  # Pause for 1 second between images\n",
    "    # clear the current image\n",
    "    clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
